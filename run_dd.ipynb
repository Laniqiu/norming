{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laniqiu/dough/blob/dd/run_dd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBqJnlP9YIMb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install pycantonese\n",
        "!pip install jiagu\n",
        "!pip install jieba"
      ],
      "metadata": {
        "id": "8DBqmz6SlVEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/dough/dd\")\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/dough/\")\n",
        "sys.path.insert(0, \"/content/drive/MyDrive/\")\n",
        "\n",
        "\n",
        "from dd_utils import *"
      ],
      "metadata": {
        "id": "90k-ngGymmJ_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from pathlib import Path\n",
        "import stanza\n",
        "from stanza.models.common.doc import Document\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "_root = \"/content/drive/MyDrive/\"\n",
        "_p = Path(_root).joinpath(\"ddata\")\n",
        "files = _p.joinpath(\"posed\").glob(\"*.txt\")\n",
        "\n",
        "def parsing_dd(files):\n",
        "    \"\"\"\n",
        "    parsing and cal dd and save to original files\n",
        "    \"\"\"\n",
        "    nlp_tra = stanza.Pipeline(lang=\"zh-hant\", \n",
        "                            processors='depparse', depparse_pretagged=True)\n",
        "    nlp_sim = stanza.Pipeline(lang=\"zh-hans\", \n",
        "                            processors='depparse', depparse_pretagged=True)\n",
        "\n",
        "    for f in files:\n",
        "        logging.info(f.name)\n",
        "        if \"simp\" in f.name:\n",
        "            lang = \"zh-hans\"  \n",
        "            nlp = nlp_sim\n",
        "        else:\n",
        "            lang =  \"zh-hant\"\n",
        "            nlp = nlp_tra\n",
        "        logging.info(\"lang:{}\".format(lang))\n",
        "        \n",
        "        cols = [\"sid\", \"wid\", \"text\", \"upos*\", \"xpos\"]\n",
        "        all_ = pd.read_table(f, sep=\"\\t\")\n",
        "        data = all_[cols].values\n",
        "\n",
        "        sents, word_id = [], []\n",
        "        sent_id = list(set(data[:,0].tolist()))\n",
        "        sent_id.sort()\n",
        "        for sid in sent_id:  # sent id\n",
        "            each_sen = []\n",
        "            ixs = np.where(data[:,0] == sid)\n",
        "            for idx, (w, p, x) in enumerate(data[:,2:][ixs]):\n",
        "                each_w = {'id': idx + 1, 'text': w, 'lemma': w, 'upos': p, \"xpos\": x}\n",
        "                each_sen.append(each_w)\n",
        "            word_id.append(data[:,1][ixs].tolist())  # word id\n",
        "            sents.append(each_sen)\n",
        "\n",
        "        doc = nlp(Document(sents))\n",
        "        logging.info(\"Done parsing\")\n",
        "        logging.info(\"Start calculating dd\")\n",
        "\n",
        "        # cal dd, 去除标点符号\n",
        "        rt_id, rt_txt = find_roots(doc)\n",
        "        roots = list(zip(rt_txt, rt_id))\n",
        "        roots = list(map(str, roots))\n",
        "\n",
        "        assert len(doc.sentences) == len(rt_id)  # 确保每一棵树都有root\n",
        "\n",
        "        dep2root, dwp = depth2root(rt_id, doc, word_id)\n",
        "        dis2root, dwr = ldd2root(rt_id, doc, word_id)\n",
        "        dis2head, dwh = ldd2head(doc, word_id)\n",
        "\n",
        "        dwri = [e[1] for l in dwr for e in l]\n",
        "        dwhi = [e[1] for l in dwh for e in l]\n",
        "        dwpi = [e[1] for l in dwp for e in l]\n",
        "\n",
        "        # 规范格式输出  save to files\n",
        "        all_[\"LD2ROOT\"] = dwri\n",
        "        all_[\"LD2HEAD\"] = dwhi\n",
        "        all_[\"DEPTH2ROOT\"] = dwpi\n",
        "        all_.to_csv(f, sep=\"\\t\") \n",
        "    "
      ],
      "metadata": {
        "id": "28gsfHHKno14"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Eeqzv1XkFumwYSo6CvYHxv8m6-m7qQH-",
      "authorship_tag": "ABX9TyOOxEkwcGwD8Reuw8XeF5r2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}